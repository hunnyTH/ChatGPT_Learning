Attention（注意力机制）：关注Encoder中Token的信息的机制，在生成每一个Token时都用到了Encoder其他Token的信息，以及它已经生成的Token的信息。  

Self-Attention：使用自注意力机制来捕捉句子中不同词语之间的依赖关系。这使得模型能够根据整个输入序列中的上下文权衡每个词语的重要性。

Parallelization并行化：与循环神经网络（RNNs）和一些其他序列模型不同，Transformer能够并行处理整个输入序列，使其在处理长距离依赖关系时更为高效。

Encoder-Decoder Structure编码器-解码器结构：Transformer通常采用编码器-解码器结构，其中编码器处理输入序列，解码器生成输出序列。这种设置特别适用于像机器翻译这样的序列到序列任务。

Transformer是Seq2Seq架构，序列到序列模型

---
![transformer](images\transformer.png "transformer框架")

### Inputs（输入）
通常是一个序列，比如一段文本的单词或标记序列。

### Outputs（输出）
也通常是一个序列，与inputs相对应。

### input Embedding & output Embedding
指将高维数据（如文本、图像或其他类型的数据）映射到低维空间的过程。

### Positional Encoding
由于Transformer不像RNNs那样固有地捕捉序列中元素的顺序，因此添加位置编码以提供关于输入序列中单词或标记位置的信息。

### Multi-Head Attention
通过同时考虑来自不同表示空间的多个注意力头（attention heads），使模型能够更好地捕捉输入序列中的关联信息。用于提高模型在处理输入序列时的表达能力和学习能力。  
* *多头机制*：输入序列会分别通过多个注意力头进行处理。每个注意力头都是一个独立的注意力机制，通过学习不同的查询、键和值的映射，来产生不同的注意力权重。
* *查询、键、值*：在每个注意力头中，输入序列首先通过线性变换得到三组不同的向量：查询（query）、键（key）和值（value）。这三个向量通常通过权重矩阵进行变换得到，以便在不同的表示空间中进行注意力计算。
* *注意力计算*：每个注意力头都会计算一个注意力分布，该分布决定了每个位置对输入序列其他位置的关注程度。注意力分布通过查询向量和键向量的内积得到，然后通过softmax函数归一化得到最终的注意力权重。
* *融合*：多个注意力头的输出会被拼接或叠加在一起，并通过另一个线性变换来得到最终的多头注意力输出。这种方式使得模型能够从不同的表示空间和不同的角度捕捉输入序列中的信息。

### Masked Multi-Head Attention
是 Transformer 模型中的一种特殊形式的多头注意力，它在处理序列时对未来位置进行掩码（mask），能够有效地应用于需要对序列进行生成和预测的任务，同时保持模型的自注意力机制在预测未来位置时的合理性和有效性。
* *掩码机制*：在进行自注意力计算时，通常会引入一个掩码矩阵，用于掩盖当前位置之后的信息。这样做的目的是为了确保在预测每个位置时，模型只能依赖其之前的信息，而不能利用当前位置之后的信息。在语言建模等任务中，这种掩码可以防止信息泄露和未来信息的利用。
* *实现方式*：在自注意力的计算中使用一个上三角形状的掩码矩阵，确保当前位置之后的信息在softmax计算中被设置为负无穷大，从而在注意力分布中不被考虑。

### Add & Norm
一种正则化和残差连接技术  
步骤：  
1. 加法连接（Addition）：  
在每个子层的输出之后，将该子层的输入（或原始输入）与子层的输出进行加法连接。这种加法连接称为残差连接（residual connection），其目的是为了使得模型可以更轻松地学习残差（即子层输出与输入之间的差异），有助于减轻梯度消失问题并加速训练。
2. 层归一化（Layer Normalization）：  
在进行加法连接后，对得到的向量进行层归一化操作。层归一化是一种归一化技术，它独立地对每个样本的特征进行归一化，以确保每个特征的均值为0、方差为1。这有助于加快模型的训练收敛速度，并且可以提升模型对输入数据的鲁棒性。

### Feed forward
1. *位置级别的处理*：
在每个注意力层之后，Transformer模型通常会包含一个前馈神经网络层。这个前馈神经网络独立地为每个位置的表示进行变换，不同位置的表示之间没有共享参数，这有助于模型更好地捕捉不同位置的语义信息。
2. *结构和参数*：
前馈神经网络通常由两个线性变换层组成，中间通过非线性激活函数（通常是ReLU）连接。具体来说，输入经过一个线性变换，然后应用激活函数，再经过另一个线性变换得到输出。这个结构使得模型能够学习到复杂的非线性映射关系。
3. *作用*：
前馈神经网络层有助于增强模型的表示能力，使得Transformer模型能够在编码器和解码器中更有效地进行特征提取和重构。在编码器中，它有助于对输入序列的特征进行编码；在解码器中，它有助于生成目标序列的预测。
4. *正则化*：
前馈神经网络层也起到了一定的正则化作用，帮助模型避免过拟合和提高泛化能力。通过使用非线性激活函数和批量归一化（如果适用），可以进一步增强模型的稳定性和收敛性。

### Linear
一个全连接层

### SoftMax
将一个K维的实数向量（通常称为 logits 或 scores）转换为一个概率分布，其中每个元素的取值范围在 0 到 1 之间，并且所有元素的和为 1。

### Output Probabilities（输出概率）
一个概率分布，表示每个可能的输出标记（如词语或标签）的概率。

---
大多数NLP任务其实并不是Seq2Seq的，最常见的主要包括这么几种：句子级别分类、Token级别分类（也叫序列标注）、相似度匹配和生成

NLU（Natural Language Understanding，自然语言理解）任务
    句子级别分类是给定一个句子，输出一个类别  
    Token级别的分类是给定一个句子，要给其中每个Token输出一个类别。  
    NLU领域的第一个工作是Google的BERT，用了Transformer的Encoder架构，有12个  Block，1亿多参数，它不预测下一个Token，而是随机把15%的Token盖住，然后利用其他没盖住的Token来预测盖住的Token。其实和根据上文预测下一个Token是类似的，不同的是可以利用下文信息。  

NLG（Natural Language Generation，自然语言生成）任务
    生成、文本摘要、机器翻译、改写纠错等  
    NLG领域的第一个工作是OpenAI的GPT，用的是Transformer的Decoder架构，参数和BERT差不多。

Transformer这个架构基于Seq2Seq，可以同时处理NLU和NLG任务，而且这种Self Attention机制的特征提取能力很强。这就使得NLP取得了阶段性的突破，深度学习开始进入了微调模型时代。大概的做法就是拿着一个开源的预训练模型，然后在自己的数据上微调一下，让它能够搞定特定的任务。


