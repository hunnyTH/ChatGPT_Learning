RLHF，Reinforcement Learning from Human Feedback，从人类反馈中学习  
# InstructGPT
用强化学习的算法微调一个根据人类反馈改进的语言模型。  
1.3B的InstructGPT堪比175B的GPT-3
流程：

* Step1：SFT，Supervised Fine-Tuning，有监督微调。顾名思义，它是在有监督（有标注）数据上微调训练得到的。这里的监督数据其实就是输入Prompt，输出相应的回复，只不过这里的回复是人工编写的。这个工作要求比一般标注要高，其实算是一种创作了。
* Step2：RM，Reward Model，奖励模型。具体来说，一个Prompt丢给前一步的SFT，输出若干个（4-9个）回复，由标注人员对这些回复进行排序。然后从4-9个中每次取2个，因为是有序的，就可以用来训练这个奖励模型，让模型学习到这个好坏评价。这一步非常关键，它就是所谓的Human Feedback，引导下一步模型的进化方向。
* Step3：RL，Reinforcement Learning，强化学习，使用PPO策略进行训练。PPO，Proximal Policy Optimization，近端策略优化，是一种强化学习优化方法，它背后的主要思想是避免每次太大的更新，提高训练的稳定性。具体过程如下：首先需要初始化一个语言模型，然后丢给它一个Prompt，它生成一个回复，上一步的RM给这个回复一个打分，这个打分回传给模型更新参数。这里的这个模型在强化学习视角下就是一个策略。这一步有个很重要的动作，就是更新模型时会考虑模型每一个Token的输出和第一步SFT输出之间的差异性，要让它俩尽量相似。这是为了缓解强化学习可能的过度优化。

强化学习在文本生成方面的研究  
难点：
一个是训练的稳定性；另一个就是奖励函数的设计。  
使用PPO策略和与SFT的差异衡量解决一  
使用了人类反馈直接作为「规则」，也就是把这种「规则」给隐式化，当做黑盒。

通用指标：有帮助、真实性和无害性

指导方针：
* 对大部分任务，无害和真实比有帮助更加重要。
* 然而，如果（a）一个输出比另一个有帮助很多；（b）该输出只是稍微不那么真实 / 无害；（c）该任务似乎不属于「高风险领域」（如贷款申请、医疗、法律咨询等）。这时候更有帮助的得分更高。
* 当选择同样有帮助但以不同方式不真实 / 有害时，问自己：哪个输出更可能对用户（在现实世界中受任务影响最大的人）造成伤害？这个输出应该排名较低。如果任务中不清楚这点，则将这些输出标记为并列。