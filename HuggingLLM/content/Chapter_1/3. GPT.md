GPT：生成式预训练Transformer  
生成式：Token by Token生成文本  
预训练：在大量语料上训练的语言模型  

### GPT-1
下游任务微调：固定住预训练模型不动，然后在不同下游任务上微调一个模型  
针对不同的任务输入，都拼接成文本序列，然后丢给Transformer Decoder再通过一个Linear+SoftMax输出结  
Linear是一种最基础的网络结构  
SoftMax主要用来把输出映射到概率分布（和为1）

基本结论：  
第一，预训练模型中的每一层都包含用于解决目标任务的有用功能，说明多层有更多能力；  
第二，随着参数的增加，Zero-Shot获得更好的性能。

简单总结来看就是，模型大了不仅能学到更多知识，有助于解决下游任务，还表现出了Zero-Shot能力。

### GPT-2

Beam Search（集束搜索）
问题：  
* 第一是生成的内容容易重复；  
* 第二是高质量的文本和高概率并不一定相关  

即生成的内容确定性太大

基于采样的方法：基于已有的上下文随机选择下一个Token  
问题：可能生成不连贯的文本  
Trick：增加高概率词的可能性，降低低概率词的可能性。  
具体的做法：用一个temperature的参数调整输出的概率分布，这个参数值越大，分布就看起来越平滑，也就是高概率和低概率的差距拉小了（对输出不那么确定）；当然越小的话，高概率和低概率的差距更明显了（对输出比较确定）。

Top-K采样：在选择下一个Token时，在Top-K个里面选（Top-K=0时就是所有词表范围）  
问题：Top-K个其实是一种硬截断，根本不管第K个概率是高还是低。极端情况下，如果某个词的概率到了0.99，K稍微大一点就必然会囊括进来一些很低概率的词。这会导致不连贯。

Top-P采样：在累计概率超过P的词里进行选择  
对于概率分布比较均匀的情况，可选的词就会多一些（可能要几十个词的概率和才会超过P）  
对于概率分布不均匀的情况，可选的词就少一些（可能2-3个词的概率就超过了P）。

任何一种采样策略都不能100%保证每一次生成的效果都很好，也没办法避免生成重复的话（可以考虑使用类似后处理的方法缓解）。也没有一种策略是在任何场景下都适用的，需要根据实际情况灵活选择。

### GPT-3
信息：
* X-Shot在不同量级差别巨大，大模型就是有超能力。  
* 大模型下，One-Shot效果明显大幅度提升；增加Prompt会进一步大幅度提升。
* Few-Shot的边际收益在递减。大概8-Shot以下时，Prompt作用明显，但从One-Shot到8-Shot，Prompt的作用也在递减。超过10-Shot时，Prompt基本没作用了。

大模型具有In-Context能力，这种能力使得它不需要针对不同任务再进行适应性训练（微调）

问题：
* 自监督训练（就是开始语言模型的方法）范式已到极限，新的方法迫在眉睫。未来的方向包括：从人类中学习目标函数、强化学习微调或多模态。
* 不确定Few-Shot是不是在推理时学习到新的任务，还是识别出来了在训练时学到的任务。最终，甚至不清楚人类从零开始学习与从之前的样本中学习分别学到什么。准确理解Few-Shot的工作原理是一个未来的方向。